{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from scipy.misc import imread, imresize\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from random import seed, choice, sample\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from cococaptioncider.pycocotools.coco import COCO\n",
    "from cococaptioncider.pycocoevalcap.eval import COCOEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get what you have, CPU or GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size):\n",
    "        super(Encoder,self).__init__()\n",
    "        #resnet = torchvision.models.resnet101(pretrained = True)\n",
    "        resnet = torchvision.models.resnet101(pretrained = True)\n",
    "        all_modules = list(resnet.children())\n",
    "        #Remove the last FC layer used for classification and the average pooling layer\n",
    "        modules = all_modules[:-2]\n",
    "        #Initialize the modified resnet as the class variable\n",
    "        self.resnet = nn.Sequential(*modules) \n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.fine_tune()    # To fine-tune the CNN, self.fine_tune(status = True)\n",
    "    \n",
    "    def forward(self,images):\n",
    "        \"\"\"\n",
    "        The forward propagation function\n",
    "        input: resized image of shape (batch_size,3,224,224)\n",
    "        \"\"\"\n",
    "        #Run the image through the ResNet\n",
    "        encoded_image = self.resnet(images)         # (batch_size,2048,7,7)\n",
    "        batch_size = encoded_image.shape[0]\n",
    "        features = encoded_image.shape[1]\n",
    "        num_pixels = encoded_image.shape[2] * encoded_image.shape[3]\n",
    "        # Get the global features of the image\n",
    "        global_features = self.avgpool(encoded_image).view(batch_size, -1)   # (batch_size, 2048)\n",
    "        enc_image = encoded_image.permute(0, 2, 3, 1)  #  (batch_size,7,7,2048)\n",
    "        enc_image = enc_image.view(batch_size,num_pixels,features)          # (batch_size,num_pixels,2048)\n",
    "        return enc_image, global_features\n",
    "    \n",
    "    def fine_tune(self, status = False):\n",
    "        \n",
    "        if not status:\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for module in list(self.resnet.children())[7:]:    #1 layer only. len(list(resnet.children())) = 8\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AdaptiveLSTMCell, self).__init__()\n",
    "        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
    "        self.x_gate = nn.Linear(input_size, hidden_size)\n",
    "        self.h_gate = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, inp, states):\n",
    "        h_old, c_old = states\n",
    "        ht, ct = self.lstm_cell(inp, (h_old, c_old))\n",
    "        sen_gate = F.sigmoid(self.x_gate(inp) + self.h_gate(h_old))\n",
    "        st =  sen_gate * F.tanh(ct)\n",
    "        return ht, ct, st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, att_dim):\n",
    "        super(AdaptiveAttention,self).__init__()\n",
    "        self.sen_affine = nn.Linear(hidden_size, hidden_size)  \n",
    "        self.sen_att = nn.Linear(hidden_size, att_dim)\n",
    "        self.h_affine = nn.Linear(hidden_size, hidden_size)   \n",
    "        self.h_att = nn.Linear(hidden_size, att_dim)\n",
    "        self.v_att = nn.Linear(hidden_size, att_dim)\n",
    "        self.alphas = nn.Linear(att_dim, 1)\n",
    "        self.context_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, spatial_image, decoder_out, st):\n",
    "        \"\"\"\n",
    "        spatial_image: the spatial image of size (batch_size,num_pixels,hidden_size)\n",
    "        decoder_out: the decoder hidden state of shape (batch_size, hidden_size)\n",
    "        st: visual sentinal returned by the Sentinal class, of shape: (batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        # view neighbor from bach_size * neighbor_num x rnn_size to bach_size x rnn_size * neighbor_num\n",
    "        num_pixels = spatial_image.shape[1]\n",
    "        visual_attn = self.v_att(spatial_image)           # (batch_size,num_pixels,att_dim)\n",
    "        sentinel_affine = F.relu(self.sen_affine(st))     # (batch_size,hidden_size)\n",
    "        sentinel_attn = self.sen_att(sentinel_affine)     # (batch_size,att_dim)\n",
    "\n",
    "        hidden_affine = F.tanh(self.h_affine(decoder_out))    # (batch_size,hidden_size)\n",
    "        hidden_attn = self.h_att(hidden_affine)               # (batch_size,att_dim)\n",
    "\n",
    "        hidden_resized = hidden_attn.unsqueeze(1).expand(hidden_attn.size(0), num_pixels + 1, hidden_attn.size(1))\n",
    "\n",
    "        concat_features = torch.cat([spatial_image, sentinel_affine.unsqueeze(1)], dim = 1)   # (batch_size, num_pixels+1, hidden_size)\n",
    "        attended_features = torch.cat([visual_attn, sentinel_attn.unsqueeze(1)], dim = 1)     # (batch_size, num_pixels+1, att_dim)\n",
    "\n",
    "        attention = F.tanh(attended_features + hidden_resized)    # (batch_size, num_pixels+1, att_dim)\n",
    "        \n",
    "        alpha = self.alphas(attention).squeeze(2)                   # (batch_size, num_pixels+1)\n",
    "        att_weights = F.softmax(alpha, dim=1)                              # (batch_size, num_pixels+1)\n",
    "\n",
    "        context = (concat_features * att_weights.unsqueeze(2)).sum(dim=1)       # (batch_size, hidden_size)     \n",
    "        beta_value = att_weights[:,-1].unsqueeze(1)                       # (batch_size, 1)\n",
    "\n",
    "        out_l = F.tanh(self.context_hidden(context + hidden_affine))\n",
    "\n",
    "        return out_l, att_weights, beta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self,hidden_size, vocab_size, att_dim, embed_size, encoded_dim):\n",
    "        super(DecoderWithAttention,self).__init__()\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.encoded_to_hidden = nn.Linear(encoded_dim, hidden_size)\n",
    "        self.global_features = nn.Linear(encoded_dim, embed_size)\n",
    "        self.LSTM = AdaptiveLSTMCell(embed_size * 2,hidden_size)\n",
    "        self.adaptive_attention = AdaptiveAttention(hidden_size, att_dim)\n",
    "        # input to the LSTMCell should be of shape (batch, input_size). Remember we are concatenating the word with\n",
    "        # the global image features, therefore out input features should be embed_size * 2\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)  \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def init_hidden_state(self, enc_image):\n",
    "        h = torch.zeros(enc_image.shape[0], 512).to(device)\n",
    "        c = torch.zeros(enc_image.shape[0], 512).to(device)\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, enc_image, global_features, encoded_captions, caption_lengths):\n",
    "        \n",
    "        \"\"\"\n",
    "        enc_image: the encoded images from the encoder, of shape (batch_size, num_pixels, 2048)\n",
    "        global_features: the global image features returned by the Encoder, of shape: (batch_size, 2048)\n",
    "        encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
    "        caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        \"\"\"\n",
    "        spatial_image = F.relu(self.encoded_to_hidden(enc_image))  # (batch_size,num_pixels,hidden_size)\n",
    "        global_image = F.relu(self.global_features(global_features))      # (batch_size,embed_size)\n",
    "        batch_size = spatial_image.shape[0]\n",
    "        num_pixels = spatial_image.shape[1]\n",
    "        # Sort input data by decreasing lengths\n",
    "        # caption_lenghts will contain the sorted lengths, and sort_ind contains the sorted elements indices \n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        #The sort_ind contains elements of the batch index of the tensor encoder_out. For example, if sort_ind is [3,2,0],\n",
    "        #then that means the descending order starts with batch number 3,then batch number 2, and finally batch number 0. \n",
    "        spatial_image = spatial_image[sort_ind]           # (batch_size,num_pixels,hidden_size) with sorted batches\n",
    "        global_image = global_image[sort_ind]             # (batch_size, embed_size) with sorted batches\n",
    "        encoded_captions = encoded_captions[sort_ind]     # (batch_size, max_caption_length) with sorted batches \n",
    "        enc_image = enc_image[sort_ind]                   # (batch_size, num_pixels, 2048)\n",
    "\n",
    "        # Embedding. Each batch contains a caption. All batches have the same number of rows (words), since we previously\n",
    "        # padded the ones shorter than max_caption_length, as well as the same number of columns (embed_dim)\n",
    "        embeddings = self.embedding(encoded_captions)     # (batch_size, max_caption_length, embed_dim)\n",
    "\n",
    "        # Initialize the LSTM state\n",
    "        h,c = self.init_hidden_state(enc_image)          # (batch_size, hidden_size)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "\n",
    "        # Create tensors to hold word predicion scores,alphas and betas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), self.vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels+1).to(device)\n",
    "        betas = torch.zeros(batch_size, max(decode_lengths),1).to(device) \n",
    "        \n",
    "        # Concatenate the embeddings and global image features for input to LSTM \n",
    "        global_image = global_image.unsqueeze(1).expand_as(embeddings)\n",
    "        inputs = torch.cat((embeddings,global_image), dim = 2)    # (batch_size, max_caption_length, embed_dim * 2)\n",
    "\n",
    "        #Start decoding\n",
    "        for timestep in range(max(decode_lengths)):\n",
    "            # Create a Packed Padded Sequence manually, to process only the effective batch size N_t at that timestep. Note\n",
    "            # that we cannot use the pack_padded_seq provided by torch.util because we are using an LSTMCell, and not an LSTM\n",
    "            batch_size_t = sum([l > timestep for l in decode_lengths])\n",
    "            current_input = inputs[:batch_size_t, timestep, :]             # (batch_size_t, embed_dim * 2)\n",
    "            h, c, st = self.LSTM(current_input, (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, hidden_size)\n",
    "            # Run the adaptive attention model\n",
    "            out_l, alpha_t, beta_t = self.adaptive_attention(spatial_image[:batch_size_t],h,st)\n",
    "            # Compute the probability over the vocabulary\n",
    "            pt = self.fc(self.dropout(out_l))                  # (batch_size, vocab_size)\n",
    "            predictions[:batch_size_t, timestep, :] = pt\n",
    "            alphas[:batch_size_t, timestep, :] = alpha_t\n",
    "            betas[:batch_size_t, timestep, :] = beta_t\n",
    "        return predictions, alphas, betas, encoded_captions, decode_lengths, sort_ind  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "checkpoint = 'BEST_checkpoint_12.pth.tar'\n",
    "checkpoint = torch.load(checkpoint)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "with open('caption data/WORDMAP_coco.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "rev_word_map = {v: k for k, v in word_map.items()}  # idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(image): \n",
    "    \"\"\"\n",
    "    Predict output with beam size of 1 (predict the word and feed it to the next LSTM). \n",
    "    Prints out the generated sentence\n",
    "    \"\"\"\n",
    "    max_len = 20\n",
    "    sampled = []\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}  # idx2word\n",
    "    img = imread(image)\n",
    "    img = imresize(img, (224, 224))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = img / 255.\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 224, 224)\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 224, 224)\n",
    "    enc_image,  global_features = encoder(image)\n",
    "    num_pix = enc_image.shape[1]\n",
    "    spatial_image = F.relu(decoder.encoded_to_hidden(enc_image))  # (batch_size,num_pixels,hidden_size)\n",
    "    global_image = F.relu(decoder.global_features(global_features))      # (batch_size,embed_size)\n",
    "    alphas_stored = torch.zeros(max_len, num_pix+1)\n",
    "    betas_stored = torch.zeros(max_len,1)\n",
    "    pred = torch.LongTensor([[word_map['<start>']]]).to(device)   # (1, 1)  \n",
    "    betas_stored = torch.zeros(max_len,1)\n",
    "    h,c = decoder.init_hidden_state(enc_image)                    #  (1,hidden_size)\n",
    "\n",
    "    for timestep in range(max_len):\n",
    "        embeddings = decoder.embedding(pred).squeeze(1)       # (1,1,embed_dim) --> (1,embed_dim)    \n",
    "        inputs = torch.cat((embeddings,global_image), dim = 1)    # (1, embed_dim * 2)\n",
    "        h, c, st = decoder.LSTM(inputs, (h, c))  # (1, hidden_size)\n",
    "        # Run the adaptive attention model\n",
    "        out, alpha, beta = decoder.adaptive_attention(spatial_image, h, st)\n",
    "        # Compute the probability\n",
    "        pt = decoder.fc(out)  \n",
    "        _,pred = pt.max(1)\n",
    "        sampled.append(pred.item())\n",
    "        alphas_stored[timestep] = alpha\n",
    "        betas_stored[timestep] = beta.item()\n",
    "        \n",
    "    generated_words = [rev_word_map[sampled[i]] for i in range(len(sampled))]\n",
    "    filtered_words = ' '.join([word for word in generated_words if word != '<end>'])\n",
    "    print(filtered_words)\n",
    "    print(betas_stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_output('test_imgs/test1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation with Beam Search\n",
    "def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n",
    "    \n",
    "    k = beam_size\n",
    "    vocab_size = len(word_map)\n",
    "    infinite_pred = False\n",
    "\n",
    "    # Read image and process\n",
    "    img = imread(image_path)\n",
    "    if len(img.shape) == 2:\n",
    "        img = img[:, :, np.newaxis]\n",
    "        img = np.concatenate([img, img, img], axis=2)\n",
    "    img = imresize(img, (224, 224))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = img / 255.\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 224, 224)\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 224, 224)\n",
    "    enc_image, global_features = encoder(image) #enc_image of shape (batch_size,num_pixels,features)\n",
    "    # Flatten encoding\n",
    "    num_pixels = enc_image.size(1)\n",
    "    encoder_dim = enc_image.size(2)\n",
    "  \n",
    "    # We'll treat the problem as having a batch size of k\n",
    "    enc_image = enc_image.expand(k, num_pixels, encoder_dim)\n",
    "    # Tensor to store top k previous words at each step; now they're just <start>\n",
    "    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "    # Tensor to store top k sequences; now they're just <start>\n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
    "    seqs_alpha = torch.ones(k, 1, 7, 7).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
    "    #Tensor to store the top k sequences betas\n",
    "    seqs_betas = torch.ones(k,1,1).to(device) \n",
    "    # Lists to store completed sequences, their alphas, betas and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_alpha = list()\n",
    "    complete_seqs_scores = list()\n",
    "    complete_seqs_betas = list()       \n",
    "    # Start decoding\n",
    "    step = 1\n",
    "    h, c = decoder.init_hidden_state(enc_image)\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "    spatial_image = F.relu(decoder.encoded_to_hidden(enc_image))  # (k,num_pixels,hidden_size)\n",
    "    global_image = F.relu(decoder.global_features(global_features))      # (1,embed_dim)\n",
    "    \n",
    "    while True:\n",
    "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (k,embed_dim)\n",
    "        inputs = torch.cat((embeddings, global_image.expand_as(embeddings)), dim = 1)    \n",
    "        h, c, st = decoder.LSTM(inputs , (h, c))  # (batch_size_t, hidden_size)\n",
    "        # Run the adaptive attention model\n",
    "        out_l, alpha, beta_t = decoder.adaptive_attention(spatial_image, h, st)\n",
    "        alpha = alpha[:,:-1]\n",
    "        alpha = alpha.view(-1, 7, 7)  # (s, enc_image_size, enc_image_size)\n",
    "        # Compute the probability over the vocabulary\n",
    "        scores = decoder.fc(out_l)      # (batch_size, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)   # (s, vocab_size)\n",
    "        # (k,1) will be (k,vocab_size), then (k,vocab_size) + (s,vocab_size) --> (s, vocab_size)\n",
    "        scores = top_k_scores.expand_as(scores) + scores  \n",
    "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            #Remember: torch.topk returns the top k scores in the first argument, and their respective indices in the second argument\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words / vocab_size  # (s) \n",
    "        next_word_inds = top_k_words % vocab_size  # (s) \n",
    "        # Add new words to sequences, alphas\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "        # (s, step+1, enc_image_size, enc_image_size)\n",
    "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],dim=1)  \n",
    "        seqs_betas = torch.cat([seqs_betas[prev_word_inds], beta_t[prev_word_inds].unsqueeze(1)], dim=1)  \n",
    "\n",
    "        # Which sequences are incomplete (didn't reach <end>)?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map['<end>']]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # Set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            complete_seqs_betas.extend(seqs_betas[complete_inds])   \n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # Proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "            \n",
    "        seqs = seqs[incomplete_inds]              \n",
    "        seqs_alpha = seqs_alpha[incomplete_inds]   \n",
    "        seqs_betas = seqs_betas[incomplete_inds]    \n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "        spatial_image = spatial_image[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 50:\n",
    "            infinite_pred = True\n",
    "            break\n",
    "            \n",
    "        step += 1\n",
    "        \n",
    "    if infinite_pred is not True:\n",
    "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        seq = complete_seqs[i]\n",
    "    else:\n",
    "        i = 0\n",
    "        seq = seqs[i][:20]\n",
    "        seq = [seq[j].item() for j in range(len(seq))]\n",
    "        \n",
    "    alphas = complete_seqs_alpha[i]\n",
    "    betas = complete_seqs_betas[i] \n",
    "\n",
    "    return seq, alphas, betas     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_att(image_path, seq, alphas, betas, rev_word_map, smooth=True):\n",
    "    \"\"\"\n",
    "    Visualizes caption with weights at every word.\n",
    "    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n",
    "    :param image_path: path to image that has been captioned\n",
    "    :param seq: caption\n",
    "    :param alphas: weights\n",
    "    :param rev_word_map: reverse word mapping, i.e. ix2word\n",
    "    :param smooth: smooth weights?\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([7 * 7, 7 * 7], Image.LANCZOS)\n",
    "    words = [rev_word_map[ind] for ind in seq]\n",
    "    print(' '.join(words[1:-1]))\n",
    "\n",
    "    for t in range(len(words)):\n",
    "        if t > 50:\n",
    "            break\n",
    "        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n",
    "        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n",
    "        plt.text(10, 65, '%.2f' % (1-(betas[t].item())), color='green', backgroundcolor='white', fontsize=15)\n",
    "        plt.imshow(image)\n",
    "        current_alpha = alphas[t, :]\n",
    "        if smooth:\n",
    "            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=7, sigma=7)\n",
    "        else:\n",
    "            alpha = skimage.transform.resize(current_alpha.numpy(), [7 * 7, 7 * 7])\n",
    "        if t == 0:\n",
    "            plt.imshow(alpha, alpha=0)\n",
    "        else:\n",
    "            plt.imshow(alpha, alpha=0.8)\n",
    "        plt.set_cmap('jet')\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7, 7)  # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "# Encode, decode with attention and beam search k=3\n",
    "seq, alphas, betas = caption_image_beam_search(encoder, decoder, 'test_imgs/test.jpg', word_map)\n",
    "alphas = torch.FloatTensor(alphas)\n",
    "# Visualize caption and attention of best sequence\n",
    "visualize_att('test_imgs/test.jpg', seq, alphas, betas, rev_word_map, smooth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
